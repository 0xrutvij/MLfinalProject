TL

Scikit's Binary Decision Trees:
For Max Depth of  1
---------------------------------------
Using Gini Impurity
028 | 002
000 | 264
Precision: 1.0
Recall: 0.9333333333333333
F1 Score: 0.9655172413793104

Using Entropy
028 | 002
000 | 264
Precision: 1.0
Recall: 0.9333333333333333
F1 Score: 0.9655172413793104

For Max Depth of  3
---------------------------------------
Using Gini Impurity
030 | 000
000 | 264
Precision: 1.0
Recall: 1.0
F1 Score: 1.0

Using Entropy
030 | 000
000 | 264
Precision: 1.0
Recall: 1.0
F1 Score: 1.0

For Max Depth of  5
---------------------------------------
Using Gini Impurity
030 | 000
000 | 264
Precision: 1.0
Recall: 1.0
F1 Score: 1.0

Using Entropy
030 | 000
000 | 264
Precision: 1.0
Recall: 1.0
F1 Score: 1.0

************************************
Scikit's Neural Networks:
For a step size of  0.001
---------------------------------------
Neural Network Using Sigmoid Activation:
026 | 004
005 | 259
Precision: 0.8387096774193549
Recall: 0.8666666666666667
F1 Score: 0.8524590163934426
Neural Network Using tanh Activation:
025 | 005
000 | 264
Precision: 1.0
Recall: 0.8333333333333334
F1 Score: 0.9090909090909091
Neural Network Using RELU Activation:
023 | 007
011 | 253
Precision: 0.6764705882352942
Recall: 0.7666666666666667
F1 Score: 0.71875

For a step size of  0.01
---------------------------------------
Neural Network Using Sigmoid Activation:
028 | 002
000 | 264
Precision: 1.0
Recall: 0.9333333333333333
F1 Score: 0.9655172413793104
Neural Network Using tanh Activation:
028 | 002
000 | 264
Precision: 1.0
Recall: 0.9333333333333333
F1 Score: 0.9655172413793104
Neural Network Using RELU Activation:
025 | 005
000 | 264
Precision: 1.0
Recall: 0.8333333333333334
F1 Score: 0.9090909090909091

For a step size of  0.1
---------------------------------------
Neural Network Using Sigmoid Activation:
027 | 003
000 | 264
Precision: 1.0
Recall: 0.9
F1 Score: 0.9473684210526316
Neural Network Using tanh Activation:
025 | 005
014 | 250
Precision: 0.6410256410256411
Recall: 0.8333333333333334
F1 Score: 0.7246376811594204
Neural Network Using RELU Activation:
026 | 004
013 | 251
Precision: 0.6666666666666666
Recall: 0.8666666666666667
F1 Score: 0.7536231884057971

For a step size of  1
---------------------------------------
Neural Network Using Sigmoid Activation:
000 | 030
000 | 264
Precision: 0
Recall: 0.0
F1 Score: 0
Neural Network Using tanh Activation:
000 | 030
000 | 264
Precision: 0
Recall: 0.0
F1 Score: 0
Neural Network Using RELU Activation:
000 | 030
000 | 264
Precision: 0
Recall: 0.0
F1 Score: 0

